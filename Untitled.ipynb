{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b459fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import final\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import re, os , sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import distance\n",
    "import tqdm \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import mlflow\n",
    "import xgboost  as xgb\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from prefect import task, flow\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    " 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",\n",
    " 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    " \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    " 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    " 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    " 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
    " 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', \n",
    "'each', 'few', 'more', 'most', 'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    " 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're',\n",
    " 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\",\n",
    " 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n",
    "\"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path:str)->pd.DataFrame:\n",
    "    try:\n",
    "        if os.path.isfile(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df = df.head(50000)\n",
    "            return df\n",
    "        else:\n",
    "            print('check tha data path')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "   \n",
    "def null_check(data=pd.DataFrame, flags=str)->pd.DataFrame:\n",
    "    try:\n",
    "        df = data\n",
    "        if df.isnull().sum().sum():\n",
    "            nan = df[df.isnull().any(1)]\n",
    "            print(nan)\n",
    "            print('-'*50)\n",
    "            if flags == 'del':\n",
    "                clean_data = df.dropna()\n",
    "            else:\n",
    "                clean_data = df.fillna(flags)\n",
    "            return clean_data\n",
    "        else:\n",
    "            \n",
    "            print('data_set has non null values')\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# funation for text preprocessing (Expanding contractions )\n",
    "def preprocess_text(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    x = re.sub(r\"http\\S+\", \"\", x)\n",
    "    x = re.sub('\\W', ' ', x) \n",
    "    bfs = BeautifulSoup(x) # removing html tage form the text\n",
    "    x = bfs.get_text()\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def data_cleaning(data):\n",
    "    data['Cl_question1'] = data['question1'].apply(preprocess_text)\n",
    "    data['Cl_question2'] = data['question2'].apply(preprocess_text)\n",
    "    return data\n",
    "\n",
    "\n",
    "# function removing stopword and stemming or lemmatizer\n",
    "\n",
    "def removing_stopword(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = data.split()\n",
    "    x = [lemmatizer.lemmatize(word) for word in data if word not in stopwords]\n",
    "    x = ' '.join(x)\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "# function for lemmitization\n",
    "\n",
    "def lemm_data(data):\n",
    "    try:\n",
    "\n",
    "        lemm = pd.DataFrame()\n",
    "        lemm['id'] = data['id']\n",
    "        lemm['lemm_data_q1'] = data.question1.apply(removing_stopword)\n",
    "        lemm['lemm_data_q2'] = data.question2.apply(removing_stopword)\n",
    "        return lemm.merge(data[['id','qid1','qid2','is_duplicate']],how='left',on='id')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "def doesMatch (q, match):\n",
    "    q1, q2 = q['question1'], q['question2']\n",
    "    q1 = str(q1).split()\n",
    "    q2 = str(q2).split()\n",
    "    if len(q1)>0 and len(q2)>0 and q1[match]==q2[match]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "    \n",
    "\n",
    "def common_stop_words_ratio(q,value):\n",
    "    q1_tokens =str( q.question1).split()\n",
    "    q2_tokens = str(q.question2).split()\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in stopwords])\n",
    "    q2_stops = set([word for word in q2_tokens if word in stopwords])\n",
    "    \n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    if value == 'min':\n",
    "        token_features = common_stop_count / (min(len(q1_stops), len(q2_stops)) + 0.0001)\n",
    "    elif value == 'max':\n",
    "        token_features = common_stop_count / (max(len(q1_stops), len(q2_stops)) + 0.0001)\n",
    "    return token_features\n",
    "\n",
    "def feature_extract(data):\n",
    "    try:\n",
    "    \n",
    "\n",
    "        print('feature_extraction_start.....')\n",
    "        data['len_q1'] = data.question1.str.len()\n",
    "        data['len_q2'] = data.question2.str.len()\n",
    "        data['q1_word'] = data.question1.apply(lambda x: len(str(x).split(' ')))\n",
    "        data['q2_word'] = data.question2.apply(lambda x: len(str(x).split(' ')))\n",
    "        \n",
    "        data['total_word'] = data['q1_word'] + data['q2_word']\n",
    "        data['differ_word_num'] = abs(data['q1_word'] - data['q2_word'])\n",
    "        data['same_first_word'] = data.apply(lambda x: doesMatch(x, 0) ,axis=1)\n",
    "        data['same_last_word'] = data.apply(lambda x: doesMatch(x, -1) ,axis=1)\n",
    "        data['total_unique_word'] = data.apply(lambda x: len(set(str(x.question1).split()).union(set(str(x.question2).split()))) ,axis=1)\n",
    "        data['total_unique_word_withoutstopword_num'] = data.apply(lambda x: len(set(str(x.question1).split()).union(set(str(x.question2).split())) - set(stopwords)) ,axis=1)\n",
    "        data['total_unique_word_num_ratio'] = data['total_unique_word'] / data['total_word']\n",
    "        print('......')\n",
    "        data['common_word'] = data.apply(lambda x: len(set(str(x.question1).split()).intersection(set(str(x.question2).split()))) ,axis=1)\n",
    "        data['common_word_ratio'] = data['common_word'] / data['total_unique_word'] # word share\n",
    "        data['word_share'] = data['common_word']/data['total_word']\n",
    "        data['common_word_ratio_min'] = data['common_word'] / data.apply(lambda x: min(len(set(str(x.question1).split())), len(set(str(x.question2).split()))) ,axis=1) \n",
    "        data['common_word_ratio_max'] = data['common_word'] / data.apply(lambda x: max(len(set(str(x.question1).split())), len(set(str(x.question2).split()))) ,axis=1) \n",
    "        \n",
    "        data['common_stop_word_ratio_min'] = common_stop_words_ratio(data,'min')\n",
    "        data['common_stop_word_ratio_max'] = common_stop_words_ratio(data, 'max')\n",
    "        \n",
    "        data['common_word_withoutstopword'] = data.apply(lambda x: len(set(str(x.question1).split()).intersection(set(str(x.question2).split())) - set(stopwords)) ,axis=1)\n",
    "        data['common_word_withoutstopword_ratio'] = data['common_word_withoutstopword'] / data['total_unique_word_withoutstopword_num']\n",
    "        \n",
    "        data['common_word_withoutstopword_ratio_min'] = data['common_word_withoutstopword'] / data.apply(lambda x: min(len(set(str(x.question1).split()) - set(stopwords)), len(set(str(x.question2).split()) - set(stopwords))) ,axis=1) \n",
    "        data['common_word_withoutstopword_ratio_max'] = data['common_word_withoutstopword'] / data.apply(lambda x: max(len(set(str(x.question1).split()) - set(stopwords)), len(set(str(x.question2).split()) - set(stopwords))) ,axis=1) \n",
    "        \n",
    "        print('fuzzy features...')\n",
    "        print('fuzz_ratio.....')\n",
    "        data[\"fuzz_ratio\"] = data.apply(lambda x: fuzz.ratio(str(x.question1), str(x.question2)), axis=1)\n",
    "        \n",
    "        print('fuzz_partial_ratio.....')\n",
    "        data[\"fuzz_partial_ratio\"] = data.apply(lambda x: fuzz.partial_ratio(str(x.question1), str(x.question2)), axis=1)\n",
    "        \n",
    "        print('fuzz_token_set_ratio.....')\n",
    "        data[\"fuzz_token_set_ratio\"] = data.apply(lambda x: fuzz.token_set_ratio(str(x.question1), str(x.question2)), axis=1)\n",
    "        \n",
    "        print('fuzz_token_sort_ratio.....')\n",
    "        data[\"fuzz_token_sort_ratio\"] = data.apply(lambda x: fuzz.token_sort_ratio(str(x.question1), str(x.question2)), axis=1)\n",
    "        \n",
    "        print('longest_substr_ratio.....')\n",
    "        data[\"longest_substr_ratio\"]  = data.apply(lambda x: get_longest_substr_ratio(str(x.question1), str(x.question2)), axis=1)\n",
    "        data.fillna(0, inplace=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#TF-ITF with word2vec\n",
    "\n",
    "#TF-ITF with word2vec\n",
    "\n",
    "def TF_word2vec_q1(data):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        questions = list(data['question1']) + list(data['question2'])\n",
    "\n",
    "        tfidf = TfidfVectorizer(lowercase=False,)\n",
    "        tfidf.fit_transform(questions)\n",
    "\n",
    "        # dict key:word and value:tf-idf score\n",
    "        word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "        vecs1 =[]\n",
    "        # tqdm is used to print the progress bar\n",
    "        print('vectorization start.....')\n",
    "        for qu1 in tqdm.tqdm(list(data['question1'])):\n",
    "            doc1 = nlp(qu1)\n",
    "            #384 is the number of dimensions of vectors\n",
    "            mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)]) \n",
    "            for word1 in doc1:\n",
    "                #word2vec\n",
    "                vec1 = word1.vector\n",
    "                #fetch df score\n",
    "                try:\n",
    "                    idf = word2tfidf[str(word1)]\n",
    "                except:\n",
    "                    idf = 0\n",
    "\n",
    "                # compute final vec\n",
    "                mean_vec1 += vec1 * idf\n",
    "            mean_vec1 = mean_vec1.mean(axis= 0)\n",
    "            vecs1.append(mean_vec1)\n",
    "\n",
    "        df['q1_feats_m'] = list(vecs1)\n",
    "        df2_q1 = pd.DataFrame(df.q1_feats_m.values.tolist(), index= data.index)\n",
    "        return df2_q1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def TF_word2vec_q2(data):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        questions = list(data['question1']) + list(data['question2'])\n",
    "\n",
    "        tfidf = TfidfVectorizer(lowercase=False,)\n",
    "        tfidf.fit_transform(questions)\n",
    "\n",
    "        # dict key:word and value:tf-idf score\n",
    "        word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "        vecs2 =[]\n",
    "        # tqdm is used to print the progress bar\n",
    "        print('vectorization start.....')\n",
    "        for qu1 in tqdm.tqdm(list(data['question2'])):\n",
    "            doc1 = nlp(qu1)\n",
    "            #384 is the number of dimensions of vectors\n",
    "            mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)]) \n",
    "            for word1 in doc1:\n",
    "                #word2vec\n",
    "                vec1 = word1.vector\n",
    "                #fetch df score\n",
    "                try:\n",
    "                    idf = word2tfidf[str(word1)]\n",
    "                except:\n",
    "                    idf = 0\n",
    "\n",
    "                # compute final vec\n",
    "                mean_vec1 += vec1 * idf\n",
    "            mean_vec1 = mean_vec1.mean(axis= 0)\n",
    "            vecs2.append(mean_vec1)\n",
    "\n",
    "        df['q1_feats_m'] = list(vecs2)\n",
    "        df2_q2 = pd.DataFrame(df.q1_feats_m.values.tolist(), index= data.index)\n",
    "        return df2_q2\n",
    "        # vecs2 = []\n",
    "        # for qu2 in tqdm.tqdm(list(data['question2'])):\n",
    "        #     doc2 = nlp(qu2) \n",
    "        #     mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n",
    "        #     for word2 in doc2:\n",
    "        #         # word2vec\n",
    "        #         vec2 = word2.vector\n",
    "        #         # fetch df score\n",
    "        #         try:\n",
    "        #             idf = word2tfidf[str(word2)]\n",
    "        #         except:\n",
    "        #             #print word\n",
    "        #             idf = 0\n",
    "        #         # compute final vec\n",
    "        #         mean_vec2 += vec2 * idf\n",
    "        #     mean_vec2 = mean_vec2.mean(axis=0)\n",
    "        #     vecs2.append(mean_vec2)\n",
    "        # data['q2_feats_m'] = list(vecs2)\n",
    "\n",
    "        df2_q2 = pd.DataFrame(data.q2_feats_m.values.tolist(), index= data.index)\n",
    "    \n",
    "        return df2_q1,df2_q2\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def marge_data(question1_vec,question2_vec,feature_data_pr):\n",
    "    try:    \n",
    "    \n",
    "        df = pd.read_csv('./train.csv')\n",
    "        question1_vec['id']=df['id']\n",
    "        question2_vec['id']=df['id']\n",
    "\n",
    "        df1  = question1_vec.merge(question2_vec, on='id',how='left')\n",
    "        final  = feature_data_pr.merge(df1, on='id',how='left')\n",
    "        return final\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def split_data(x,y,test_ratio):\n",
    "    try:\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_ratio)\n",
    "        return x_train,x_test,y_train,y_test\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "def model_train(x_train,y_train,model:object, param:dict):\n",
    "\n",
    "    mlflow.set_tracking_uri('sqlite:///mlflow5.db')\n",
    "    mlflow.set_experiment('Quora_pair_question_problem5')\n",
    "    mlflow.sklearn.autolog(max_tuning_runs=None)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        param_grid = param\n",
    "        \n",
    "        xg = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv = 5,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            return_train_score= True,\n",
    "            n_jobs = 1\n",
    "        )\n",
    "        xg.fit(x_train,y_train)\n",
    "        \n",
    "        #disabling autologging\n",
    "        mlflow.sklearn.autolog(disable=True)\n",
    "        print(xg.best_params_)\n",
    "        t = xg.best_params_\n",
    "    return model(learning_rate=t['learning_rate'],max_depth=t['max_depth'],min_child_weight=['min_child_weight'],n_estimators=t['n_estimators'])\n",
    "\n",
    "\n",
    "def main(path:str,data_number:int):\n",
    "    df_num = data_number\n",
    "    # Define parameters\n",
    "    Target_column = 'is_duplicate'\n",
    "    data_path = path\n",
    "\n",
    "    # load the data\n",
    "    dataframe = load_data(data_path)\n",
    "    dataframe = dataframe.head(df_num)\n",
    "\n",
    "    # data cleaning\n",
    "    clean_data = data_cleaning(dataframe)\n",
    "\n",
    "    # feature extraction \n",
    "    extract_data = feature_extract(clean_data)\n",
    "\n",
    "    # word2vec\n",
    "    question_1 = TF_word2vec_q1(clean_data)\n",
    "    question_2 = TF_word2vec_q2(clean_data)\n",
    "\n",
    "    # merg dataset into one file\n",
    "    final_data = marge_data(question_1,question_2, extract_data)\n",
    "    final_data.drop(['qid1','qid2','question1','question2','Cl_question1','Cl_question2'],axis =1 , inplace=True)\n",
    "\n",
    "    # Identify target varible\n",
    "    input_data = final_data['is_duplicate']\n",
    "    target_data =final_data\n",
    "\n",
    "\n",
    "    # split the data into train and test\n",
    "    x_train,x_test,y_train,y_test = split_data(target_data, input_data,test_ratio=0.25) \n",
    "\n",
    "    # model training\n",
    "    param_ = {  \n",
    "              'n_estimators': [40, 60, 80], \n",
    "              'max_depth': range(1, 4), \n",
    "              'learning_rate': [1e-3], \n",
    "              'min_child_weight': range(1, 4), \n",
    "             }\n",
    "    model = model_train(x_train,y_train,xgb.XGBClassifier(),param=param_)\n",
    "    y_pre = model.predict(x_test)\n",
    "    print(accuracy_score(y_pre,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1eaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('./train.csv')\n",
    "data = data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40a8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = data_cleaning(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfa0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extraction_start.....\n",
      "......\n",
      "fuzzy features...\n",
      "fuzz_ratio.....\n",
      "fuzz_partial_ratio.....\n",
      "fuzz_token_set_ratio.....\n",
      "fuzz_token_sort_ratio.....\n",
      "longest_substr_ratio.....\n"
     ]
    }
   ],
   "source": [
    "extra_data = feature_extract(pre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8449753f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>Cl_question1</th>\n",
       "      <th>Cl_question2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>...</th>\n",
       "      <th>common_stop_word_ratio_max</th>\n",
       "      <th>common_word_withoutstopword</th>\n",
       "      <th>common_word_withoutstopword_ratio</th>\n",
       "      <th>common_word_withoutstopword_ratio_min</th>\n",
       "      <th>common_word_withoutstopword_ratio_max</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>fuzz_token_set_ratio</th>\n",
       "      <th>fuzz_token_sort_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>93</td>\n",
       "      <td>0.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the story of kohinoor  koh i noor  dia...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>63</td>\n",
       "      <td>0.442308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>45</td>\n",
       "      <td>41</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i mentally very lonely  how can i solve it</td>\n",
       "      <td>find the remainder when  math 23  24   math  i...</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>which one dissolve in water quikly sugar  salt...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>37</td>\n",
       "      <td>54</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                        Cl_question1  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor  koh i noor  dia...   \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely  how can i solve it   \n",
       "4  which one dissolve in water quikly sugar  salt...   \n",
       "\n",
       "                                        Cl_question2  len_q1  len_q2  ...  \\\n",
       "0  what is the step by step guide to invest in sh...      66      57  ...   \n",
       "1  what would happen if the indian government sto...      51      88  ...   \n",
       "2  how can internet speed be increased by hacking...      73      59  ...   \n",
       "3  find the remainder when  math 23  24   math  i...      50      65  ...   \n",
       "4             which fish would survive in salt water      76      39  ...   \n",
       "\n",
       "   common_stop_word_ratio_max  common_word_withoutstopword  \\\n",
       "0                    0.555552                            5   \n",
       "1                    0.555552                            3   \n",
       "2                    0.555552                            2   \n",
       "3                    0.555552                            0   \n",
       "4                    0.555552                            1   \n",
       "\n",
       "   common_word_withoutstopword_ratio  common_word_withoutstopword_ratio_min  \\\n",
       "0                           0.625000                               0.833333   \n",
       "1                           0.250000                               0.600000   \n",
       "2                           0.166667                               0.333333   \n",
       "3                           0.000000                               0.000000   \n",
       "4                           0.062500                               0.166667   \n",
       "\n",
       "   common_word_withoutstopword_ratio_max  fuzz_ratio  fuzz_partial_ratio  \\\n",
       "0                               0.714286          93                  98   \n",
       "1                               0.300000          65                  73   \n",
       "2                               0.250000          45                  41   \n",
       "3                               0.000000           7                  20   \n",
       "4                               0.090909          37                  54   \n",
       "\n",
       "   fuzz_token_set_ratio  fuzz_token_sort_ratio  longest_substr_ratio  \n",
       "0                   100                     93              0.965517  \n",
       "1                    86                     63              0.442308  \n",
       "2                    63                     63              0.150000  \n",
       "3                    28                     24              0.039216  \n",
       "4                    67                     47              0.150000  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c78b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 ['id']=extra_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26ff345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        2\n",
       "3        3\n",
       "4        4\n",
       "      ... \n",
       "495    495\n",
       "496    496\n",
       "497    497\n",
       "498    498\n",
       "499    499\n",
       "Name: id, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_1['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6f9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c60225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization start.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 89.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization start.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 91.73it/s] \n"
     ]
    }
   ],
   "source": [
    "question_1 = TF_word2vec_q1(clean_data)\n",
    "question_2 = TF_word2vec_q2(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cda9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    " final_data = marge_data(question_1,question_2, extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2abd783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>Cl_question1</th>\n",
       "      <th>Cl_question2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>...</th>\n",
       "      <th>86_y</th>\n",
       "      <th>87_y</th>\n",
       "      <th>88_y</th>\n",
       "      <th>89_y</th>\n",
       "      <th>90_y</th>\n",
       "      <th>91_y</th>\n",
       "      <th>92_y</th>\n",
       "      <th>93_y</th>\n",
       "      <th>94_y</th>\n",
       "      <th>95_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>23.454691</td>\n",
       "      <td>0.674020</td>\n",
       "      <td>-7.689485</td>\n",
       "      <td>8.017969</td>\n",
       "      <td>-40.982749</td>\n",
       "      <td>-44.132399</td>\n",
       "      <td>36.545455</td>\n",
       "      <td>26.094779</td>\n",
       "      <td>8.286586</td>\n",
       "      <td>-10.927646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the story of kohinoor  koh i noor  dia...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>10.007542</td>\n",
       "      <td>2.172399</td>\n",
       "      <td>-21.943437</td>\n",
       "      <td>7.306729</td>\n",
       "      <td>-7.361206</td>\n",
       "      <td>-29.931989</td>\n",
       "      <td>-2.977271</td>\n",
       "      <td>11.206990</td>\n",
       "      <td>37.379291</td>\n",
       "      <td>-1.079838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>14.705095</td>\n",
       "      <td>-13.560906</td>\n",
       "      <td>10.911867</td>\n",
       "      <td>26.596330</td>\n",
       "      <td>-48.137169</td>\n",
       "      <td>-50.337705</td>\n",
       "      <td>19.173741</td>\n",
       "      <td>-9.433019</td>\n",
       "      <td>15.220369</td>\n",
       "      <td>-0.265939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>why am i mentally very lonely  how can i solve it</td>\n",
       "      <td>find the remainder when  math 23  24   math  i...</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103875</td>\n",
       "      <td>3.048119</td>\n",
       "      <td>9.530259</td>\n",
       "      <td>9.200311</td>\n",
       "      <td>-31.301418</td>\n",
       "      <td>-22.883158</td>\n",
       "      <td>-11.503366</td>\n",
       "      <td>-13.039456</td>\n",
       "      <td>9.274324</td>\n",
       "      <td>-33.010861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>which one dissolve in water quikly sugar  salt...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>6.619158</td>\n",
       "      <td>6.906724</td>\n",
       "      <td>-20.575831</td>\n",
       "      <td>14.353661</td>\n",
       "      <td>-7.385053</td>\n",
       "      <td>-20.996447</td>\n",
       "      <td>20.379570</td>\n",
       "      <td>-9.969032</td>\n",
       "      <td>19.916848</td>\n",
       "      <td>-0.429476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>988</td>\n",
       "      <td>989</td>\n",
       "      <td>What is the painting on this image?</td>\n",
       "      <td>What is this painting?</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the painting on this image</td>\n",
       "      <td>what is this painting</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>4.943079</td>\n",
       "      <td>2.973088</td>\n",
       "      <td>-0.541268</td>\n",
       "      <td>9.095978</td>\n",
       "      <td>-6.569571</td>\n",
       "      <td>-9.005506</td>\n",
       "      <td>6.117695</td>\n",
       "      <td>-10.918294</td>\n",
       "      <td>6.973176</td>\n",
       "      <td>-4.480150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>990</td>\n",
       "      <td>991</td>\n",
       "      <td>Which are the major highways in California and...</td>\n",
       "      <td>Which are the major highways in California and...</td>\n",
       "      <td>0</td>\n",
       "      <td>which are the major highways in california and...</td>\n",
       "      <td>which are the major highways in california and...</td>\n",
       "      <td>104</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>21.169208</td>\n",
       "      <td>10.416384</td>\n",
       "      <td>-29.275103</td>\n",
       "      <td>13.112648</td>\n",
       "      <td>-27.348557</td>\n",
       "      <td>-16.100905</td>\n",
       "      <td>1.341413</td>\n",
       "      <td>-12.765768</td>\n",
       "      <td>8.850809</td>\n",
       "      <td>1.817075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>992</td>\n",
       "      <td>993</td>\n",
       "      <td>What's beyond our Universe?</td>\n",
       "      <td>If space is expanding, where does the new spac...</td>\n",
       "      <td>0</td>\n",
       "      <td>what is beyond our universe</td>\n",
       "      <td>if space is expanding  where does the new spac...</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>18.016417</td>\n",
       "      <td>30.344229</td>\n",
       "      <td>-5.382860</td>\n",
       "      <td>26.147936</td>\n",
       "      <td>-11.395241</td>\n",
       "      <td>-21.436577</td>\n",
       "      <td>2.073962</td>\n",
       "      <td>-22.975147</td>\n",
       "      <td>31.533559</td>\n",
       "      <td>-22.074005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>994</td>\n",
       "      <td>995</td>\n",
       "      <td>Is growing of hair a physical or a chemical ch...</td>\n",
       "      <td>Can a bald person ever grow their hair back?</td>\n",
       "      <td>0</td>\n",
       "      <td>is growing of hair a physical or a chemical ch...</td>\n",
       "      <td>can a bald person ever grow their hair back</td>\n",
       "      <td>51</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.986114</td>\n",
       "      <td>13.469428</td>\n",
       "      <td>15.255748</td>\n",
       "      <td>2.418254</td>\n",
       "      <td>-17.205735</td>\n",
       "      <td>-37.589467</td>\n",
       "      <td>0.772018</td>\n",
       "      <td>-11.253944</td>\n",
       "      <td>26.276028</td>\n",
       "      <td>-13.192669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>996</td>\n",
       "      <td>997</td>\n",
       "      <td>What is the difference between culture and his...</td>\n",
       "      <td>How is chili different between cultures?</td>\n",
       "      <td>0</td>\n",
       "      <td>what is the difference between culture and his...</td>\n",
       "      <td>how is chili different between cultures</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>11.695086</td>\n",
       "      <td>-5.037005</td>\n",
       "      <td>-22.643080</td>\n",
       "      <td>7.061111</td>\n",
       "      <td>-4.617968</td>\n",
       "      <td>-21.834231</td>\n",
       "      <td>6.175120</td>\n",
       "      <td>6.200373</td>\n",
       "      <td>11.054153</td>\n",
       "      <td>-25.524578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  qid1  qid2                                          question1  \\\n",
       "0      0     1     2  What is the step by step guide to invest in sh...   \n",
       "1      1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      2     5     6  How can I increase the speed of my internet co...   \n",
       "3      3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "..   ...   ...   ...                                                ...   \n",
       "495  495   988   989                What is the painting on this image?   \n",
       "496  496   990   991  Which are the major highways in California and...   \n",
       "497  497   992   993                        What's beyond our Universe?   \n",
       "498  498   994   995  Is growing of hair a physical or a chemical ch...   \n",
       "499  499   996   997  What is the difference between culture and his...   \n",
       "\n",
       "                                             question2  is_duplicate  \\\n",
       "0    What is the step by step guide to invest in sh...             0   \n",
       "1    What would happen if the Indian government sto...             0   \n",
       "2    How can Internet speed be increased by hacking...             0   \n",
       "3    Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4              Which fish would survive in salt water?             0   \n",
       "..                                                 ...           ...   \n",
       "495                             What is this painting?             0   \n",
       "496  Which are the major highways in California and...             0   \n",
       "497  If space is expanding, where does the new spac...             0   \n",
       "498       Can a bald person ever grow their hair back?             0   \n",
       "499           How is chili different between cultures?             0   \n",
       "\n",
       "                                          Cl_question1  \\\n",
       "0    what is the step by step guide to invest in sh...   \n",
       "1    what is the story of kohinoor  koh i noor  dia...   \n",
       "2    how can i increase the speed of my internet co...   \n",
       "3    why am i mentally very lonely  how can i solve it   \n",
       "4    which one dissolve in water quikly sugar  salt...   \n",
       "..                                                 ...   \n",
       "495                 what is the painting on this image   \n",
       "496  which are the major highways in california and...   \n",
       "497                        what is beyond our universe   \n",
       "498  is growing of hair a physical or a chemical ch...   \n",
       "499  what is the difference between culture and his...   \n",
       "\n",
       "                                          Cl_question2  len_q1  len_q2  ...  \\\n",
       "0    what is the step by step guide to invest in sh...      66      57  ...   \n",
       "1    what would happen if the indian government sto...      51      88  ...   \n",
       "2    how can internet speed be increased by hacking...      73      59  ...   \n",
       "3    find the remainder when  math 23  24   math  i...      50      65  ...   \n",
       "4               which fish would survive in salt water      76      39  ...   \n",
       "..                                                 ...     ...     ...  ...   \n",
       "495                              what is this painting      35      22  ...   \n",
       "496  which are the major highways in california and...     104     100  ...   \n",
       "497  if space is expanding  where does the new spac...      27      58  ...   \n",
       "498        can a bald person ever grow their hair back      51      44  ...   \n",
       "499            how is chili different between cultures      51      40  ...   \n",
       "\n",
       "          86_y       87_y       88_y       89_y       90_y       91_y  \\\n",
       "0    23.454691   0.674020  -7.689485   8.017969 -40.982749 -44.132399   \n",
       "1    10.007542   2.172399 -21.943437   7.306729  -7.361206 -29.931989   \n",
       "2    14.705095 -13.560906  10.911867  26.596330 -48.137169 -50.337705   \n",
       "3     0.103875   3.048119   9.530259   9.200311 -31.301418 -22.883158   \n",
       "4     6.619158   6.906724 -20.575831  14.353661  -7.385053 -20.996447   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "495   4.943079   2.973088  -0.541268   9.095978  -6.569571  -9.005506   \n",
       "496  21.169208  10.416384 -29.275103  13.112648 -27.348557 -16.100905   \n",
       "497  18.016417  30.344229  -5.382860  26.147936 -11.395241 -21.436577   \n",
       "498  -8.986114  13.469428  15.255748   2.418254 -17.205735 -37.589467   \n",
       "499  11.695086  -5.037005 -22.643080   7.061111  -4.617968 -21.834231   \n",
       "\n",
       "          92_y       93_y       94_y       95_y  \n",
       "0    36.545455  26.094779   8.286586 -10.927646  \n",
       "1    -2.977271  11.206990  37.379291  -1.079838  \n",
       "2    19.173741  -9.433019  15.220369  -0.265939  \n",
       "3   -11.503366 -13.039456   9.274324 -33.010861  \n",
       "4    20.379570  -9.969032  19.916848  -0.429476  \n",
       "..         ...        ...        ...        ...  \n",
       "495   6.117695 -10.918294   6.973176  -4.480150  \n",
       "496   1.341413 -12.765768   8.850809   1.817075  \n",
       "497   2.073962 -22.975147  31.533559 -22.074005  \n",
       "498   0.772018 -11.253944  26.276028 -13.192669  \n",
       "499   6.175120   6.200373  11.054153 -25.524578  \n",
       "\n",
       "[500 rows x 227 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.merge(extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85203343",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()\n",
    "final_data.drop(['qid1','qid2','question1','question2','Cl_question1','Cl_question2'],axis =1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c197bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2387819",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_column = 'is_duplicate'\n",
    "input_data = final_data[Target_column]\n",
    "target_data =final_data.drop(['is_duplicate'],axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = split_data(target_data, input_data,test_ratio=0.25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1164a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ = {  \n",
    "              'n_estimators': [40, 60, 80], \n",
    "              'max_depth': range(1, 4), \n",
    "              'learning_rate': [1e-3], \n",
    "              'min_child_weight': range(1, 4), \n",
    "             }\n",
    "model = model_train(x_train,y_train,xgb.XGBClassifier(),param=param_)\n",
    "y_pre = model.predict(x_test)\n",
    "print(accuracy_score(y_pre,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c285f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost  as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a054a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c0aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
